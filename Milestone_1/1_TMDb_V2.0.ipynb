{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTDS'18 milestone 1: network collection and properties\n",
    "[Effrosyni Simou](https://lts4.epfl.ch/simou), [EPFL LTS4](https://lts4.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `10`\n",
    "* Students: `Fluhr Hugo, Donzier Paul, De Goumoens Frédéric, Cionca Alexandre`\n",
    "* Dataset: `IMDb, tmdb_5000_movies.csv and tmdb_5000_credits.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to three sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this milestone is to start getting acquainted to the network that you will use for this class. In the first part of the milestone you will import your data using [Pandas](http://pandas.pydata.org) and you will create the adjacency matrix using [Numpy](http://www.numpy.org). This part is project specific. In the second part you will have to compute some basic properties of your network. **For the computation of the properties you are only allowed to use the packages that have been imported in the cell below.** You are not allowed to use any graph-specific toolboxes for this milestone (such as networkx and PyGSP). Furthermore, the aim is not to blindly compute the network properties, but to also start to think about what kind of network you will be working with this semester. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Import your data and manipulate them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A. Load your data in a Panda dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you should define and understand what are your nodes, what features you have and what are your labels. Please provide below a Panda dataframe where each row corresponds to a node with its features and labels. For example, in the the case of the Free Music Archive (FMA) Project, each row of the dataframe would be of the following form:\n",
    "\n",
    "\n",
    "| Track   |  Feature 1  | Feature 2 | . . . | Feature 518|  Label 1 |  Label 2 |. . .|Label 16|\n",
    "|:-------:|:-----------:|:---------:|:-----:|:----------:|:--------:|:--------:|:---:|:------:|\n",
    "|         |             |           |       |            |          |          |     |        |\n",
    "\n",
    "It is possible that in some of the projects either the features or the labels are not available. This is OK, in that case just make sure that you create a dataframe where each of the rows corresponds to a node and its associated features or labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DATA LOADING\n",
    "\n",
    "# We load both datasets, credits contains the casts of the movies and movies contains movie features\n",
    "credits=pd.read_csv('../dataset/TMDb/tmdb_5000_credits.csv')\n",
    "movies=pd.read_csv('../dataset/TMDb/tmdb_5000_movies.csv')\n",
    "\n",
    "# The columns cast and crew of the dataset credits are strings, we use json.loads to convert into dict\n",
    "json_columns = ['cast', 'crew']\n",
    "for column in json_columns:\n",
    "     credits[column] = credits[column].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# DATA FILTERING\n",
    "\n",
    "# Here we fuse the 2 datasets into one and drop all columns useless for this milestone\n",
    "movie_df=credits.join(movies, rsuffix='other')\n",
    "movie_df=movie_df.drop(columns=['budget','overview','original_language','vote_average','crew',\n",
    "                            'popularity','genres','homepage','id','keywords','original_title',\n",
    "                            'production_companies','production_countries','release_date',\n",
    "                            'revenue','runtime','spoken_languages','tagline','titleother'])\n",
    "\n",
    "# To reduce the dataset, we filter unreleased movies and movies with a vote count inferior to 100\n",
    "movie_df=movie_df[movie_df.status=='Released']\n",
    "movie_df=movie_df[movie_df.vote_count >=100]\n",
    "movie_df=movie_df.drop(columns=['vote_count','status'])\n",
    "\n",
    "# Reseting the index after filtering\n",
    "movie_df=movie_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "      <th>actors_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19995</td>\n",
       "      <td>Avatar</td>\n",
       "      <td>[65731, 8691, 10205, 32747, 17647, 1771, 59231...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>285</td>\n",
       "      <td>Pirates of the Caribbean: At World's End</td>\n",
       "      <td>[85, 114, 116, 1640, 1619, 2440, 118, 1709, 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>206647</td>\n",
       "      <td>Spectre</td>\n",
       "      <td>[8784, 27319, 121529, 5469, 28782, 17064, 2038...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49026</td>\n",
       "      <td>The Dark Knight Rises</td>\n",
       "      <td>[3894, 3895, 64, 1813, 2524, 8293, 24045, 192,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49529</td>\n",
       "      <td>John Carter</td>\n",
       "      <td>[60900, 21044, 2206, 5293, 19159, 2983, 8785, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movie_id                                     title  \\\n",
       "0     19995                                    Avatar   \n",
       "1       285  Pirates of the Caribbean: At World's End   \n",
       "2    206647                                   Spectre   \n",
       "3     49026                     The Dark Knight Rises   \n",
       "4     49529                               John Carter   \n",
       "\n",
       "                                           actors_id  \n",
       "0  [65731, 8691, 10205, 32747, 17647, 1771, 59231...  \n",
       "1  [85, 114, 116, 1640, 1619, 2440, 118, 1709, 24...  \n",
       "2  [8784, 27319, 121529, 5469, 28782, 17064, 2038...  \n",
       "3  [3894, 3895, 64, 1813, 2524, 8293, 24045, 192,...  \n",
       "4  [60900, 21044, 2206, 5293, 19159, 2983, 8785, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating empty column which will be filled with the id of the actors playing in each movie\n",
    "movie_df['actors_id']=\"\"\n",
    "\n",
    "# Extraction of actors ID from the cast dictionnary and adding them to the added column\n",
    "for i in range(len(movie_df.title)):\n",
    "    strs=[]\n",
    "    for ii in range(len(movie_df.cast[i])):\n",
    "        strs.append(movie_df.cast[i][ii].get('id'))\n",
    "    movie_df.at[i,'actors_id']=strs\n",
    "\n",
    "# The \"cast\" column is no longer needed\n",
    "movie_df = movie_df.drop(columns='cast')\n",
    "\n",
    "# Preview of the Milestone 1 dataframe\n",
    "movie_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the adjacency matrix of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there are edges connecting the attributed nodes that you organized in the dataframe above. The connectivity of the network is captured by the adjacency matrix $W$. If $N$ is the number of nodes, the adjacency matrix is an $N \\times N$ matrix where the value of $W(i,j)$ is the weight of the edge connecting node $i$ to node $j$.  \n",
    "\n",
    "There are two possible scenarios for your adjacency matrix construction, as you already learned in the tutorial by Benjamin:\n",
    "\n",
    "1) The edges are given to you explicitly. In this case you should simply load the file containing the edge information and parse it in order to create your adjacency matrix. See how to do that in the  [graph from edge list]() demo.\n",
    "\n",
    "2) The edges are not given to you. In that case you will have to create a feature graph. In order to do that you will have to chose a distance that will quantify how similar two nodes are based on the values in their corresponding feature vectors. In the [graph from features]() demo Benjamin showed you how to build feature graphs when using Euclidean distances between feature vectors. Be curious and explore other distances as well! For instance, in the case of high-dimensional feature vectors, you might want to consider using the cosine distance. Once you compute the distances between your nodes you will have a fully connected network. Do not forget to sparsify by keeping the most important edges in your network.\n",
    "\n",
    "Follow the appropriate steps for the construction of the adjacency matrix of your network and provide it in the Numpy array ``adjacency`` below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-06f23c4f6b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0madjacency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactors_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactors_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# We normalize by the maximum number of shared actors between two movies found in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ntds_2018/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ntds_2018/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3115\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'getitem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n",
      "\u001b[0;32m~/miniconda3/envs/ntds_2018/lib/python3.7/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return (super(Int64Index, self)\n\u001b[0;32m--> 187\u001b[0;31m                 ._convert_scalar_indexer(key, kind=kind))\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_joined_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ntds_2018/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_scalar_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'positional'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCMultiIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0;31m# we can raise here if we are definitive that this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ntds_2018/lib/python3.7/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mRangeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \"\"\"\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of nodes in the network\n",
    "n_nodes = len(movie_df)\n",
    "\n",
    "# The adjacency matrix contains the number of common actors for each pair of nodes (movies)\n",
    "# The matrix is set to be symmetric since the relation of shared actor is undirected\n",
    "adjacency=np.zeros((n_nodes,n_nodes), dtype=float)\n",
    "\n",
    "for i in range(len(movie_df.index)):\n",
    "    for ii in range(i+1,(len(movie_df.index))):\n",
    "            adjacency[i,ii]=adjacency[ii,i]=len(set(movie_df.actors_id[i]).intersection(set(movie_df.actors_id[ii])))\n",
    "            \n",
    "# We normalize by the maximum number of shared actors between two movies found in the dataset\n",
    "adjacency = np.divide(adjacency,adjacency.max());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see from the adjacency matrix that there are some movies that don't have any connections to any other movies,\n",
    "# these nodes don't present any interest to us so we remove them from both the dataframe and the adjacency matrix\n",
    "i=0\n",
    "while i < len(adjacency):\n",
    "    if not any(adjacency[i]):\n",
    "        adjacency = np.delete(adjacency, i, 0)\n",
    "        adjacency = np.delete(adjacency, i, 1)\n",
    "        movie_df=movie_df.drop([i])\n",
    "        movie_df=movie_df.reset_index(drop=True)\n",
    "    else:\n",
    "        i+=1\n",
    "        \n",
    "# We need to update n_nodes since we have dropped some movies\n",
    "n_nodes=len(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to plot the (weighted) adjacency matrix of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dots represent connections between movies regardless of their weight\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.spy(adjacency, markersize=0.1)\n",
    "plt.title('adjacency matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the maximum number of links $L_{max}$ in a network with $N$ nodes (where $N$ is the number of nodes in your network)? How many links $L$ are there in your collected network? Comment on the sparsity of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We find in the first lecture of the course the expression of L_max, the maximum number of links for a fully connected graph.\n",
    "# L max is equal to the number of distinct pair of nodes present in the graph\n",
    "L_max=int(n_nodes*(n_nodes-1)/2)\n",
    "print(L_max)\n",
    "\n",
    "# To compute L, we need to count how many non-zero elements there are in the top-right half of the adjacency matrix\n",
    "L=0\n",
    "for i in range(adjacency.shape[0]):\n",
    "    for ii in range(i,adjacency.shape[1]):\n",
    "        L+=(adjacency[i][ii]!=0)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_{max}$ for our graph which has 3138 nodes is equal to 4'921'953\n",
    "\n",
    "In our graph, there are 163'621 links\n",
    "\n",
    "We find a density: $L/L_{max}$ = 0.033, showing that our graph, much like many real graphs, is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Is your graph directed or undirected? If it is directed, convert it to an undirected graph by symmetrizing the adjacency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph is undirected. The links between movies represent the number of actors they share which is an undirected relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "In the cell below save the features dataframe and the **symmetrized** adjacency matrix. You can use the Pandas ``to_csv`` to save the ``features`` and Numpy's ``save`` to save the ``adjacency``. We will reuse those in the following milestones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving of both dataframe and adjacency matrix\n",
    "movie_df.to_csv('saved_features', sep='\\t', encoding='utf-8')\n",
    "np.save('saved_adjacency',adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Are the edges of your graph weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the edges are weighted, the weight is proportional to the number of shared actors.\n",
    "Since we have normalized the weights, they represent a proportion of how many actors two movies share divided by the maximum of shared actors between two movies found in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the degree distibution of your network? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an array containing the degree of each node\n",
    "degree = np.sum(adjacency,0)\n",
    "\n",
    "# Here we verify that there are as many elements in the degree array as there are nodes (movies) in the graph\n",
    "assert len(degree) == n_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to see the histogram of the degree distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = np.ones_like(degree) / float(n_nodes)\n",
    "plt.hist(degree, weights=weights, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the average degree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3771979154950618\n"
     ]
    }
   ],
   "source": [
    "average_deg=np.divide(np.sum(degree),n_nodes)\n",
    "print(average_deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Comment on the degree distribution of your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from the degree histogram that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an unweighted version ot the adjacency matrix which we will use in several of the following questions\n",
    "unw_adj=np.where(adjacency != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "Write a function that takes as input the adjacency matrix of a graph and determines whether the graph is connected or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We implement a version of the BFS algorithm in an additional function that we will need for both questions 7 and 8\n",
    "\n",
    "def bfs(adjacency, source_node):\n",
    "    # Explored is an array where explored[a_certain_node]= 0 when this a_certain_node cannot be reached by source node\n",
    "    # or = 1 when this a_certain_node can be reached\n",
    "    n_nodes=len(adjacency)\n",
    "    explored=np.full(n_nodes,0)\n",
    "    # Store nodes to explore in a second list\n",
    "    to_explore = [source_node]\n",
    "    \n",
    "    # The source node is already explored\n",
    "    explored[source_node] = 1  \n",
    "    \n",
    "    # Stay in loop while there are still nodes to explore\n",
    "    while to_explore:\n",
    "       # Pop first node frome the queue and add it to to the explored nodes\n",
    "        current_node = to_explore.pop(0)\n",
    "        explored[current_node] = 1\n",
    "        # Copy the line of the matrix corresponding to the current node to see its connections\n",
    "        neighbours = np.argwhere(adjacency[current_node]).flatten()\n",
    "        \n",
    "        # Set the neighbours of current_node to be explored\n",
    "        for i in range(len(neighbours)):\n",
    "            if not explored[neighbours[i]]:\n",
    "                to_explore.append(neighbours[i])\n",
    "                explored[neighbours[i]]=1\n",
    "    \n",
    "    return explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connected_graph(adjacency):\n",
    "    \"\"\"Determines whether a graph is connected.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the graph is connected, False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # We call the previously implemented bfs function and check if the number of reached nodes starting from the\n",
    "    # first node is equal to the number of nodes in the graph\n",
    "    n_nodes=len(adjacency)\n",
    "    connected = (len(np.argwhere(bfs(adjacency,0)))==n_nodes)\n",
    "    \n",
    "    return connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is your graph connected? Run the ``connected_graph`` function to determine your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connected_graph(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Write a function that extracts the connected components of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring a function that removes both column and row full of 0s from a matrix (numpy array)\n",
    "# This function is used in find_components()\n",
    "def no_zero(matrix):\n",
    "    \n",
    "    i=0\n",
    "    while i < len(matrix):\n",
    "        if not any(matrix[i]):\n",
    "            matrix = np.delete(matrix, i, 0)\n",
    "            matrix = np.delete(matrix, i, 1)\n",
    "        else:\n",
    "            i+=1\n",
    "    \n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_components(adjacency):\n",
    "    \"\"\"Find the connected components of a graph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of numpy arrays\n",
    "        A list of adjacency matrices, one per connected component.\n",
    "    \"\"\"\n",
    "\n",
    "    n_nodes=len(adjacency)\n",
    "    components=[]\n",
    "    # We declare an array of size n_nodes which contains for each node its component index,\n",
    "    # e.g. to which component it belongs\n",
    "    comp_idx=np.full(n_nodes,0)\n",
    "    component_label=1\n",
    "    \n",
    "    # The implemented bfs() function returns an array of size (n_nodes,1) which contains a one if the node\n",
    "    # is reachable from source and a zero if it isn't. This is the indicator vector of the connected\n",
    "    # component in  which is the first node (in our case). Each iteration of the while loop labels all nodes\n",
    "    # of the i-th connected component with a i.\n",
    "    while len(np.argwhere(comp_idx))<n_nodes:\n",
    "        comp_idx +=component_label*bfs(adjacency,np.argwhere(comp_idx==0)[0,0])\n",
    "        component_label+=1\n",
    "    \n",
    "    # We now extract one adjacency matrix per component and store them in the \"components\" list\n",
    "    for i in range(1,component_label):\n",
    "        matrix=adjacency@np.diag(comp_idx == i)\n",
    "        matrix=no_zero(matrix)\n",
    "        components.append(matrix)\n",
    "    \n",
    "    return components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many connected components is your network composed of? What is the size of the largest connected component? Run the ``find_components`` function to determine your answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19ffb546b106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of connected components:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'component(s)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'find_components' is not defined"
     ]
    }
   ],
   "source": [
    "comp = find_components(adjacency)\n",
    "size = 0\n",
    "\n",
    "print('Number of connected components:',len(comp), 'component(s)')\n",
    "\n",
    "# Finding the largest connected component\n",
    "for mat in comp:\n",
    "    if len(mat) > size:\n",
    "        biggest = mat\n",
    "        size = len(biggest)\n",
    "\n",
    "print('Size of the largest connected component:',len(biggest), 'node(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "Write a function that takes as input the adjacency matrix and a node (`source`) and returns the length of the shortest path between that node and all nodes in the graph using Dijkstra's algorithm. **For the purposes of this assignment we are interested in the hop distance between nodes, not in the sum of weights. **\n",
    "\n",
    "Hint: You might want to mask the adjacency matrix in the function ``compute_shortest_path_lengths`` in order to make sure you obtain a binary adjacency matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shortest_path_lengths(adjacency, source):\n",
    "    \"\"\"Compute the shortest path length between a source node and all nodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    source: int\n",
    "        The source node. A number between 0 and n_nodes-1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of ints\n",
    "        The length of the shortest path from source to all nodes. Returned list should be of length n_nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Firstly, we transform the matrix to an unweighted one\n",
    "    adjacency=np.where(adjacency != 0, 1, 0)\n",
    "    n_nodes=adjacency.shape[0]\n",
    "    \n",
    "    # All shortest path are initialy set to infinity\n",
    "    shortest_path_lengths=np.full(n_nodes,np.inf)\n",
    "    \n",
    "    shortest_path_lengths[source]=0\n",
    "    # Creation of a list containing the index of every unvisited_node\n",
    "    unvis_nodes=list(range(n_nodes))\n",
    "    \n",
    "    # We stay in the loop as long as there are nodes that hasn't been visited\n",
    "    while unvis_nodes:\n",
    "        dist_min=np.inf\n",
    "        current_node=None\n",
    "        \n",
    "        # Search for the unvisited node with the shortest shortest_path_length\n",
    "        for i in range(len(unvis_nodes)):\n",
    "            if (shortest_path_lengths[unvis_nodes[i]]<dist_min): \n",
    "                dist_min=shortest_path_lengths[unvis_nodes[i]]\n",
    "                current_node=unvis_nodes[i]\n",
    "        \n",
    "        # Remove the current_node from the unvisited_nodes\n",
    "        unvis_nodes.remove(current_node)\n",
    "        \n",
    "        # We take the neighbours of current_node that are still unvisited\n",
    "        unvis_neighbours=np.intersect1d(np.argwhere(adjacency[current_node]),np.asarray(unvis_nodes))\n",
    "        \n",
    "        # Check for each unvisited_nodes if there is an alt_path going through current_node shorter\n",
    "        # than the previously stored shortest_path_length, if there is, update the spl with alt_path\n",
    "        for i in range(len(unvis_neighbours)):\n",
    "            alt_path=shortest_path_lengths[current_node]+1\n",
    "            if (shortest_path_lengths[unvis_neighbours[i]]>alt_path):\n",
    "                shortest_path_lengths[unvis_neighbours[i]]=alt_path\n",
    "    \n",
    "    # Casting into a list of int\n",
    "    shortest_path_lengths=shortest_path_lengths.tolist()\n",
    "    shortest_path_lengths = [int(el) for el in shortest_path_lengths]\n",
    "                \n",
    "    return shortest_path_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing all shortest paths from the first node (movie)\n",
    "print(compute_shortest_path_lengths(adjacency,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "The diameter of the graph is the length of the longest shortest path between any pair of nodes. Use the above developed function to compute the diameter of the graph (or the diameter of the largest connected component of the graph if the graph is not connected). If your graph (or largest connected component) is very large, computing the diameter will take very long. In that case downsample your graph so that it has 1.000 nodes. There are many ways to reduce the size of a graph. For the purposes of this milestone you can chose to randomly select 1.000 nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing adjacency matrix size to 1000 nodes and taking the largest connected component on this reduced matrix\n",
    "red_comp = find_components(adjacency[:1000,:1000])\n",
    "size = 0\n",
    "\n",
    "for mat in red_comp:\n",
    "    if (len(mat)>size):\n",
    "        red_mat = mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-6cafd6e371e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdiameter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_shortest_path_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mred_mat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-66df0c6a5d24>\u001b[0m in \u001b[0;36mcompute_shortest_path_lengths\u001b[0;34m(adjacency, source)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0munvis_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0munvis_neighbours\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersect1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_node\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munvis_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munvis_neighbours\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ntds_2018/lib/python3.7/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "diameter=0\n",
    "\n",
    "# Finding the longest shortest path in the graph\n",
    "for i in range(n_nodes):\n",
    "    diameter=max(max(compute_shortest_path_lengths(red_mat,i)),diameter)\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "Write a function that takes as input the adjacency matrix, a path length, and two nodes (`source` and `target`), and returns the number of paths of the given length between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paths(adjacency, source, target, length):\n",
    "    \"\"\"Compute the number of paths of a given length between a source and target node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    source: int\n",
    "        The source node. A number between 0 and n_nodes-1.\n",
    "    target: int\n",
    "        The target node. A number between 0 and n_nodes-1.\n",
    "    length: int\n",
    "        The path length to be considered.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Once again, taking the unweighted adjacency matrix\n",
    "    adj_bin=np.where(adjacency != 0, 1, 0)\n",
    "    pow_adj=adj_bin\n",
    "    \n",
    "    # Compute adj_bin to the power length\n",
    "    if length==1:\n",
    "        n_paths=adj_bin[source,target]\n",
    "    else:    \n",
    "        for i in range(length-1):\n",
    "            pow_adj=np.matmul(pow_adj,adj_bin)\n",
    "        n_paths=pow_adj[source,target]   \n",
    "    \n",
    "    return n_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function on 5 pairs of nodes, with different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_paths(adjacency, 0, 10, 1))\n",
    "print(compute_paths(adjacency, 0, 10, 2))\n",
    "print(compute_paths(adjacency, 0, 10, 3))\n",
    "print(compute_paths(adjacency, 23, 67, 2))\n",
    "print(compute_paths(adjacency, 15, 93, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "How many paths of length 3 are there in your graph? Hint: calling the `compute_paths` function on every pair of node is not an efficient way to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5540e9549ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sum elements of Adjacency^3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0madj_cub\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munw_adj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munw_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munw_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_cub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Sum elements of Adjacency^3\n",
    "\n",
    "adj_cub=np.matmul(np.matmul(unw_adj,unw_adj),unw_adj)\n",
    "\n",
    "print(np.sum(np.sum(np.int64(adj_cub))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "Write a function that takes as input the adjacency matrix of your graph (or of the largest connected component of your graph) and a node and returns the clustering coefficient of that node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clustering_coefficient(adjacency, node):\n",
    "    \"\"\"Compute the clustering coefficient of a node.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    adjacency: numpy array\n",
    "        The (weighted) adjacency matrix of a graph.\n",
    "    node: int\n",
    "        The node whose clustering coefficient will be computed. A number between 0 and n_nodes-1.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The clustering coefficient of the node. A number between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Number of links between the neighbours of node on which we want to calculate the clustering coeff.\n",
    "    Li = 0\n",
    "    # List of neighbours of node on which we want to calculate the clustering coeff.\n",
    "    ki_list = []\n",
    "\n",
    "    for i in range(len(adjacency)):\n",
    "        # Find neighboring nodes\n",
    "        if (adjacency[node,i] != 0 and i != node):\n",
    "            # Store neighbouring nodes\n",
    "            ki_list.append(i)\n",
    "            \n",
    "    # If the number of neighbours is equal to or less than 1, the clustering coefficient is 0 :       \n",
    "    if (len(ki_list))<=1:\n",
    "        return 0.0\n",
    "    else :\n",
    "        # For every neighbour\n",
    "        for i in range(len(ki_list)):\n",
    "            for j in ki_list:\n",
    "                # Count every links between the neighbours only once\n",
    "                if ((adjacency[ki_list[i],j] != 0) and (j in ki_list[i:])):\n",
    "                    # No difference in the execution when giving unweighted or weighted adjacency matrix\n",
    "                    Li += 1\n",
    "        # Forumla given in lecture\n",
    "        clustering_coefficient = (2*Li)/(len(ki_list)*(len(ki_list) - 1))\n",
    "    \n",
    "    return clustering_coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "What is the average clustering coefficient of your graph (or of the largest connected component of your graph if your graph is disconnected)? Use the function ``compute_clustering_coefficient`` to determine your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_clustering_coeff = 0.\n",
    "for i in range(n_nodes):\n",
    "    average_clustering_coeff += compute_clustering_coefficient(adjacency, i)/n_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
