{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [NTDS'18] milestone 3: spectral graph theory\n",
    "[ntds'18]: https://github.com/mdeff/ntds_2018\n",
    "\n",
    "[Michaël Defferrard](http://deff.ch), [EPFL LTS2](https://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "\n",
    "* Team: `10`\n",
    "* Students: `Cionca Alexandre, De Goumoëns Frédéric, Donzier Paul, Fluhr Hugo`\n",
    "* Dataset: `TMDb dataset (subset of IMDb)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "* Milestones have to be completed by teams. No collaboration between teams is allowed.\n",
    "* Textual answers shall be short. Typically one to two sentences.\n",
    "* Code has to be clean.\n",
    "* You cannot import any other library than we imported.\n",
    "* When submitting, the notebook is executed and the results are stored. I.e., if you open the notebook again it should show numerical results and plots. We won't be able to execute your notebooks.\n",
    "* The notebook is re-executed from a blank state before submission. That is to be sure it is reproducible. You can click \"Kernel\" then \"Restart & Run All\" in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "The goal of this milestone is to get familiar with the graph Laplacian and its spectral decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Load your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a `No module named 'sklearn'` error when running the below cell, install [scikit-learn](https://scikit-learn.org) with `conda install scikit-learn` (after activating the `ntds_2018` environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "#for testing, to be removed\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import pygsp as mdeff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('../Milestone_1/fits.csv')\n",
    "\n",
    "# Importing our dataset that will be updated as adjacency is updated so that we can use it to answer Question 13 (is our clustering representing\n",
    "# some feature of the movies?)\n",
    "json_columns = ['genres','crew','production_companies','production_countries']\n",
    "for column in json_columns:\n",
    "     features[column] = features[column].apply(json.loads)\n",
    "        \n",
    "        \n",
    "features['genres_name']=\"\"\n",
    "\n",
    "for i in range(len(features.title)):\n",
    "    strs=[]\n",
    "    for ii in range(len(features.genres[i])):\n",
    "        strs.append(features.genres[i][ii].get('name'))\n",
    "    features.at[i,'genres_name']=strs\n",
    "\n",
    "#list(features) used to determine which columns to drop\n",
    "features = features.drop(columns='Unnamed: 0')\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote your graph as $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}, A)$, where $\\mathcal{V}$ is the set of nodes, $\\mathcal{E}$ is the set of edges, $A \\in \\mathbb{R}^{N \\times N}$ is the (weighted) adjacency matrix, and $N = |\\mathcal{V}|$ is the number of nodes.\n",
    "\n",
    "Import the adjacency matrix $A$ that you constructed in the first milestone.\n",
    "(You're allowed to update it between milestones if you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency = np.load(\"../Milestone_1/adj_act.npy\")\n",
    "\n",
    "n_nodes =   len(adjacency)\n",
    "print(n_nodes,'nodes')\n",
    "#adjacency[adjacency<0.05]=0\n",
    "\n",
    "#adjacency=np.where(adjacency!=0,1.,0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see from the adjacency matrix that there are some movies that don't have any connections to any other movies,\n",
    "# these nodes don't present any interest to us so we remove them from both the dataframe and the adjacency matrix\n",
    "deg=np.sum(adjacency,0)\n",
    "deg_bin=np.argwhere(deg==0).flatten()\n",
    "\n",
    "adjacency_red = np.delete(adjacency,deg_bin,0)\n",
    "adjacency_red = np.delete(adjacency_red,deg_bin,1)\n",
    "\n",
    "fits_red=features.drop(deg_bin)\n",
    "fits_red=fits_red.reset_index(drop=True)\n",
    "\n",
    "# We need to update n_nodes since we have dropped some movies\n",
    "n_nodes=len(adjacency_red)\n",
    "n_nodes\n",
    "adjacency=adjacency_red\n",
    "len(adjacency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Graph Laplacian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "From the (weighted) adjacency matrix $A$, compute both the combinatorial (also called unnormalized) and the normalized graph Laplacian matrices.\n",
    "\n",
    "Note: if your graph is weighted, use the weighted adjacency matrix. If not, use the binary adjacency matrix.\n",
    "\n",
    "For efficient storage and computation, store these sparse matrices in a [compressed sparse row (CSR) format](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_.28CSR.2C_CRS_or_Yale_format.29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create the degree vector of our graph :\n",
    "degrees= np.sum(adjacency,0)\n",
    "\n",
    "D_sqrt_inv= np.diag(1/np.sqrt(degrees))\n",
    "\n",
    "laplacian_combinatorial = sparse.csr_matrix(np.diag(degrees)-adjacency)\n",
    "laplacian_normalized = sparse.csr_matrix(D_sqrt_inv @ laplacian_combinatorial @ D_sqrt_inv)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.spy(laplacian_combinatorial,markersize=0.1)\n",
    "plt.subplot(122)\n",
    "plt.spy(laplacian_normalized,markersize=0.1)\n",
    "lap_un=laplacian_combinatorial.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use one of them as the graph Laplacian $L$ for the rest of the milestone.\n",
    "We however encourage you to run the code with both to get a sense of the difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laplacian =  laplacian_combinatorial\n",
    "laplacian = laplacian_normalized\n",
    "\n",
    "# We chose the normalized laplacian because its useful to get the number of clusters (see question 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Compute the eigendecomposition of the Laplacian $L = U^\\top \\Lambda U$, where the columns $u_k \\in \\mathbb{R}^N$ of $U = [u_1, \\dots, u_N] \\in \\mathbb{R}^{N \\times N}$ are the eigenvectors and the diagonal elements $\\lambda_k = \\Lambda_{kk}$ are the corresponding eigenvalues.\n",
    "\n",
    "Make sure that the eigenvalues are ordered, i.e., $0 = \\lambda_1 \\leq \\lambda_2 \\leq \\dots \\leq \\lambda_N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scipy linalg (not scipy.sparse.linagl!!!)\n",
    "vals_norm,vects_norm = scipy.linalg.eigh(laplacian.toarray())\n",
    "print(vals_norm[0])\n",
    "\n",
    "# Numpy linalg\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())\n",
    "print(eigenvalues[0])\n",
    "\n",
    "vals_un,vects_un = np.linalg.eigh(lap_un)\n",
    "\n",
    "#assert eigenvectors.shape == (n_nodes, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit vals_norm,vects_norm = scipy.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit eigenvalues, eigenvectors = np.linalg.eigh(laplacian.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(eigenvectors[:,0])\n",
    "#print(vects[:,0])\n",
    "sum(vects_norm[:,0]**2) #gives the norm of the vector which is equal to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justify your choice of eigensolver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "https://stackoverflow.com/questions/6684238/whats-the-fastest-way-to-find-eigenvalues-vectors-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We can write $L = S S^\\top$. What is the matrix $S$? What does $S^\\top x$, with $x \\in \\mathbb{R}^N$, compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S is the incidence matrix. It contains the information about the orientation of the edges, it gives a correspondace between\n",
    "vertices and edges. S is of size N x M, N being number of vertices and M number of edges. S(i,j)= +1 / -1 if node i is \n",
    "the outgoing / ingoing node of edge j, S(i,j)= 0 othewise.\n",
    "$S^\\top x$ computes the gradient of x. It gives the derivative of x at each edge, $S^\\top x$ is a vector of size M."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Show that $\\lambda_k = \\| S^\\top u_k \\|_2^2$, where $\\| \\cdot \\|_2^2$ denotes the squared Euclidean norm (a.k.a. squared $L^2$ norm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering that $L = S S^\\top $ and that $\\| S^\\top u_k \\|_2^2 = u_k^\\top S S^\\top u_k  $. We use the definition of eigenvalues and eigenvectors :\n",
    "\n",
    "$L u_k = \\lambda_k u_k$\n",
    "\n",
    "Left-multiplying both sides by $u_k^\\top$ gives :\n",
    "\n",
    "$ u_k^\\top L u_k = u_k^\\top \\lambda_k u_k$\n",
    "\n",
    "$\\lambda_k $ being a scalar we can rewrite :\n",
    "\n",
    "$ u_k^\\top L u_k = \\lambda_k u_k^\\top  u_k$\n",
    "\n",
    "Assuming that the eigenvectors are normalized (which is the case for most eigensolvers) we get :\n",
    "\n",
    "$ u_k^\\top L u_k = \\lambda_k $\n",
    "\n",
    "Which is equivalent to :\n",
    "\n",
    "$ \\lambda_k = u_k^\\top L u_k = u_k^\\top S S^\\top u_k = \\| S^\\top u_k \\|_2^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the quantity $\\| S^\\top x \\|_2^2$ tell us about $x$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quantity is the square of the norm of the gradient of x, meaning that it tells us how much the signal x varies over the graph.\n",
    "It is an indicator of the smoothness of x over the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "What is the value of $u_0$, both for the combinatorial and normalized Laplacians?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects_norm[:,0] # for normalized, same as combinatorial but normalized by degree? in lecture : L*u = lambda*D*u \n",
    "vects_un[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your annswer here.**\n",
    "\n",
    "Should be indicating vector of giant component, so full of 1's (normalized) for our network which is connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Look at the spectrum of the Laplacian by plotting the eigenvalues.\n",
    "Comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eig_min=0\n",
    "eig_max=20\n",
    "x = range(eig_min,eig_max,1)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "\n",
    "plt.subplot(121)\n",
    "#plt.plot(vals)\n",
    "plt.scatter(x,vals_norm[eig_min:eig_max])\n",
    "#plt.scatter(vals[:exe],list(np.ones((exe,1))))\n",
    "\n",
    "plt.subplot(122)\n",
    "#plt.plot(eigenvalues)\n",
    "plt.scatter(x,eigenvalues[eig_min:eig_max])\n",
    "#plt.scatter(eigenvalues[:exe],list(np.ones((exe,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many connected components are there in your graph? Answer using the eigenvalues only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "# 1 since first eigenvalue is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there an upper bound on the eigenvalues, i.e., what is the largest possible eigenvalue? Answer for both the combinatorial and normalized Laplacians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Laplacian eigenmaps\n",
    "\n",
    "*Laplacian eigenmaps* is a method to embed a graph $\\mathcal{G}$ in a $d$-dimensional Euclidean space.\n",
    "That is, it associates a vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$.\n",
    "The graph $\\mathcal{G}$ is thus embedded as $Z \\in \\mathbb{R}^{N \\times d}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use our bfs to get the index of the nodes that are part of the giant component (attaignable from node 0)\n",
    "\n",
    "def bfs(adjacency, source_node):\n",
    "    # Explored is an array where explored[a_certain_node]= 0 when this a_certain_node cannot be reached by source node\n",
    "    # or = 1 when this a_certain_node can be reached\n",
    "    n_nodes=len(adjacency)\n",
    "    explored=np.full(n_nodes,0)\n",
    "    # Store nodes to explore in a list\n",
    "    to_explore = [source_node]\n",
    "    \n",
    "    # The source node is already explored\n",
    "    explored[source_node] = 1  \n",
    "    \n",
    "    # Stay in loop while there are still nodes to explore\n",
    "    while to_explore:\n",
    "       # Pop first node frome the queue and add it to to the explored nodes\n",
    "        current_node = to_explore.pop(0)\n",
    "        explored[current_node] = 1\n",
    "        # Copy the line of the matrix corresponding to the current node to see its connections\n",
    "        neighbours = np.argwhere(adjacency[current_node]).flatten()\n",
    "        \n",
    "        # Set the neighbours of current_node to be explored\n",
    "        for i in range(len(neighbours)):\n",
    "            if not explored[neighbours[i]]:\n",
    "                to_explore.append(neighbours[i])\n",
    "                explored[neighbours[i]]=1\n",
    "    \n",
    "    return explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects_un[:,1]\n",
    "idx=np.where(np.abs(vects_un[:,1]) <1e-3 , 1, 0)\n",
    "bf=np.where(bfs(adjacency,0)==0)\n",
    "# create an array containing the index of the nodes which are not in the giant component\n",
    "idx_non_giant=np.argwhere(1-idx)\n",
    "len(idx_non_giant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_red = np.delete(adjacency,idx_non_giant,0)\n",
    "adjacency_red = np.delete(adjacency_red,idx_non_giant,1)\n",
    "feat=fits_red\n",
    "feat=feat.drop(idx_non_giant.flatten())\n",
    "feat=feat.reset_index(drop=True)\n",
    "len(adjacency_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Graph = nx.from_numpy_array(adjacency)\n",
    "Gr_2 = nx.from_numpy_array(adjacency_red)\n",
    "\n",
    "giant=max(nx.connected_component_subgraphs(Graph), key=len)\n",
    "print(nx.is_connected(giant))\n",
    "print(nx.is_connected(Gr_2))\n",
    "\n",
    "\n",
    "adjacency_gc=np.asarray(nx.to_numpy_matrix(giant))\n",
    "print(np.max(adjacency_red-adjacency_gc))\n",
    "\n",
    "#nx.write_gexf(Gr_2, 'movies_keywords.gexf')\n",
    "\n",
    "##print(nx.connected_components(Graph))\n",
    "len(adjacency_gc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees=np.sum(adjacency_red,0)\n",
    "D_sqrt_inv= np.diag(1/np.sqrt(degrees))\n",
    "\n",
    "laplacian_combinatorial = sparse.csr_matrix(np.diag(degrees)-adjacency_red)\n",
    "laplacian_normalized = sparse.csr_matrix(D_sqrt_inv @ laplacian_combinatorial @ D_sqrt_inv)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(121)\n",
    "plt.spy(laplacian_combinatorial,markersize=0.1)\n",
    "plt.subplot(122)\n",
    "plt.spy(laplacian_normalized,markersize=0.1)\n",
    "laplacian=laplacian_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What do we use Laplacian eigenmaps for? (Or more generally, graph embeddings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "Embed in an Euclidian Space, for visualisation for example, the visualisation has to \"show\" properties of the graph\n",
    "Laplacian Eigenmaps are also useful for dimensionality reduction using neighborhood information of the dataset (each data point is a node of the graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Embed your graph in $d=2$ dimensions with Laplacian eigenmaps.\n",
    "Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "\n",
    "**Recompute** the eigenvectors you need with a partial eigendecomposition method for sparse matrices.\n",
    "When $k \\ll N$ eigenvectors are needed, partial eigendecompositions are much more efficient than complete eigendecompositions.\n",
    "A partial eigendecomposition scales as $\\Omega(k |\\mathcal{E}|$), while a complete eigendecomposition costs $\\mathcal{O}(N^3)$ operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create the degree vector of our graph :\n",
    "degrees=np.sum(adjacency_gc,0)\n",
    "D_sqrt_inv= np.diag(1/np.sqrt(degrees))\n",
    "\n",
    "laplacian_combinatorial = sparse.csr_matrix(np.diag(degrees)-adjacency_gc)\n",
    "laplacian_normalized = sparse.csr_matrix(D_sqrt_inv @ laplacian_combinatorial @ D_sqrt_inv)\n",
    "\n",
    "n_eig=13\n",
    "\n",
    "# Eigenvalues and eigenvectors are computed using the \n",
    "vals_s, eigenvectors_s = sparse.linalg.eigsh(laplacian_normalized,n_eig,maxiter=400000, which='SM')\n",
    "\n",
    "# Eigenvectors matrix is Re-normalized by the degrees\n",
    "eigenV = D_sqrt_inv @ eigenvectors_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_min=0\n",
    "n_max=30\n",
    "ran=range(len(vals_s))\n",
    "plt.scatter(ran[n_min:n_max],vals_s[n_min:n_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k=1\n",
    "\n",
    "plt.figure(figsize=(20,80))\n",
    "\n",
    "for i in range(1, len(eigenvectors_s[0])-1,1):\n",
    "    for j in range(i+1,len(eigenvectors_s[0])-1,1):\n",
    "        #x=eigenvectors_s[:,i]\n",
    "        #y=eigenvectors_s[:,j]\n",
    "        x=eigenV[:,i]\n",
    "        y=eigenV[:,j]\n",
    "        plt.subplot(20,5,k)\n",
    "        strange='i:'+ str(i) +'j:'+ str(j)\n",
    "        plt.title(strange)\n",
    "        plt.scatter(x, y, alpha=0.25, c=K_means.labels_, cmap='hsv')\n",
    "        k+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the nodes embedded in 2D. Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "x1= eigenvectors_s[:,1]\n",
    "y1= eigenvectors_s[:,9]\n",
    "x2= eigenvectors_s[:,4]\n",
    "y2= eigenvectors_s[:,10]\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.scatter(x1, y1, alpha=0.25, c=K_means.labels_, cmap='hsv');\n",
    "plt.title(\"No re-normalization (1,9)\")\n",
    "plt.xlabel(\"2nd Eigenvector\")\n",
    "plt.ylabel(\"9th Eigenvector\")\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.scatter(x2, y2, alpha=0.25, c=K_means.labels_, cmap='hsv');\n",
    "plt.title(\"No re-normalization (4,10)\")\n",
    "plt.xlabel(\"11th Eigenvector\")\n",
    "plt.ylabel(\"9th Eigenvector\")\n",
    "\n",
    "x1= eigenV[:,1]\n",
    "y1= eigenV[:,9]\n",
    "x2= eigenV[:,4]\n",
    "y2= eigenV[:,10]\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.scatter(x1, y1, alpha=0.25, c=K_means.labels_, cmap='hsv');\n",
    "plt.title(\"Re-normalization (1,9)\")\n",
    "plt.xlabel(\"2nd Eigenvector\")\n",
    "plt.ylabel(\"9th Eigenvector\")\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.scatter(x2, y2, alpha=0.25, c=K_means.labels_, cmap='hsv');\n",
    "plt.title(\"Re-normalization (4,10)\")\n",
    "plt.xlabel(\"11th Eigenvector\")\n",
    "plt.ylabel(\"9th Eigenvector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the embedding $Z \\in \\mathbb{R}^{N \\times d}$ preserve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**\n",
    "\n",
    "LES DISTANCES\n",
    "\n",
    "A COMPLETER <br>\n",
    "Points close to each other on the manifold are mapped close to each other in the low-dimensional space, preserving local distances.\n",
    "https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Laplacian_eigenmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Spectral clustering\n",
    "\n",
    "*Spectral clustering* is a method to partition a graph into distinct clusters.\n",
    "The method associates a feature vector $z_i \\in \\mathbb{R}^d$ to every node $v_i \\in \\mathcal{V}$, then runs [$k$-means](https://en.wikipedia.org/wiki/K-means_clustering) in the embedding space $\\mathbb{R}^d$ to assign each node $v_i \\in \\mathcal{V}$ to a cluster $c_j \\in \\mathcal{C}$, where $k = |\\mathcal{C}|$ is the number of desired clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Choose $k$ and $d$. How did you get to those numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose k from observing the spectrum of our graph. As we saw in the lecture, the index of the eigenvalue (of the normalized Laplacian) at which there is a gap (a discontinuity in the spectrum) is a good choice for the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "1. Embed your graph in $\\mathbb{R}^d$ as $Z \\in \\mathbb{R}^{N \\times d}$.\n",
    "   Try with and without re-normalizing the eigenvectors by the degrees, then keep the one your prefer.\n",
    "1. If you want $k=2$ clusters, partition with the Fiedler vector. For $k > 2$ clusters, run $k$-means on $Z$. Don't implement $k$-means, use the `KMeans` class imported from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eigV=9\n",
    "k=10\n",
    "\n",
    "#H=eigenvectors_s[:,1:n_eigV]\n",
    "H=eigenV[:,1:n_eigV]\n",
    "\n",
    "print(np.shape(H))\n",
    "\n",
    "K_means=KMeans(n_clusters = k, random_state=0).fit(H)\n",
    "\n",
    "labs=[]\n",
    "\n",
    "for i in range(k):\n",
    "    labs.append(len(np.argwhere(K_means.labels_ == i)))\n",
    "    print(i,':',labs[i])\n",
    "\n",
    "labs=sorted(labs, reverse=True)\n",
    "labels=K_means.labels_\n",
    "## turn a graph into a set of vectors and run k-means on it,\n",
    "## rows are samples, columns are features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "\n",
    "Use the computed cluster assignment to reorder the adjacency matrix $A$.\n",
    "What do you expect? What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "## order by clusters, the order is no longer arbitrary, comment on wether this shows a structure in the network\n",
    "## maybe the clusters will give sequels? MCU, Pirates of the carribeans...\n",
    "## the clusters number order doesn't matter\n",
    "sort_by_cluster=np.argsort(labels)\n",
    "feat['cluster'] = labels\n",
    "feat['index_by_cluster']=np.argsort(sort_by_cluster)\n",
    "feat.sort_values('index_by_cluster')\n",
    "\n",
    "adjacency_sorted=adjacency_red[sort_by_cluster,:]\n",
    "adjacency_sorted=adjacency_sorted[:,sort_by_cluster]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.spy(adjacency_sorted, markersize=0.1)\n",
    "plt.title('adjacency matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat.sort_values('cluster',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "\n",
    "If you have ground truth clusters for your dataset, compare the cluster assignment from spectral clustering to the ground truth.\n",
    "A simple quantitative measure is to compute the percentage of nodes that have been correctly categorized.\n",
    "If you don't have a ground truth, qualitatively assess the quality of the clustering.\n",
    "\n",
    "Ground truth clusters are the \"real clusters\".\n",
    "For example, the genre of musical tracks in FMA, the category of Wikipedia articles, the spammer status of individuals, etc.\n",
    "Look for the `labels` in the [dataset descriptions](https://github.com/mdeff/ntds_2018/tree/master/projects/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "\n",
    "Plot the cluster assignment (one color per cluster) on the 2D embedding you computed above with Laplacian eigenmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "\n",
    "Why did we use the eigenvectors of the graph Laplacian as features? Could we use other features for clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
